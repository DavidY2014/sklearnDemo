# sklearnDemo

准备工具，使用jupyter环境
参考：
https://www.jianshu.com/p/061c6e5c4b0d   Jupyter Notebook 入门指南

熵：表示随机变量的不确定性。条件熵：在一个条件下，随机变量的不确定性。信息增益：熵 - 条件熵在一个条件下，信息不确定性减少的程度！通俗地讲，X(明天下雨)是一个随机变量，X的熵可以算出来， Y(明天阴天)也是随机变量，在阴天情况下下雨的信息熵我们如果也知道的话（此处需要知道其联合概率分布或是通过数据估计）即是条件熵。X的熵减去Y条件下X的熵，就是信息增益。具体解释：原本明天下雨的信息熵是2，条件熵是0.01（因为如果是阴天就下雨的概率很大，信息就少了），这样相减后为1.99。在获得阴天这个信息后，下雨信息不确定性减少了1.99，不确定减少了很多，所以信息增益大。也就是说，阴天这个信息对下雨来说是很重要的。所以在特征选择的时候常常用信息增益，如果IG（信息增益大）的话那么这个特征对于分类来说很关键，决策树就是这样来找特征的


所谓最大似然估计，就是假设硬币的参数，然后计算实验结果的概率是多少，概率越大的，那么这个假设的参数就越可能是真的。


对条件概率公式进行变形，可以得到如下形式：


    我们把P(A)称为"先验概率"（Prior probability），即在B事件发生之前，我们对A事件概率的一个判断。

    P(A|B)称为"后验概率"（Posterior probability），即在B事件发生之后，我们对A事件概率的重新评估。

    P(B|A)/P(B)称为"可能性函数"（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。

    所以，条件概率可以理解成下面的式子
————————————————
版权声明：本文为CSDN博主「Jack-Cui」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/c406495762/article/details/77341116







